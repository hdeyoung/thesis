\chapter{Introduction}\label{ch:introduction}

\fixnote{Simple example throughout introduction?}

\begin{itemize}
\item
  Concurrency is notoriously difficult to get right.
  \begin{itemize}
  \item For functional programming, Curry--Howard application of proof theory is the gold standard.
  \item Since Girard, efforts to explain concurrency using proof theory.
  \end{itemize}

\item 
  Two proof-theoretic ways to characterize concurrency:
  \begin{itemize}
  \item proof construction as concurrency
  \item proof reduction as concurrency
  \end{itemize}
  Each has its own strengths and weaknesses.
  Relatively little (none?) study of the relationship between these characterizations.

\item
  Proof reduction $\cong$ session-typed processes (SILL).
  \begin{itemize}
  \item Strengths: behavioral types; preservation and progress; process implementation;
                   logical extensions suggest new features
  \item Weaknesses: very local description (cf.\ local and global session types);
                    synchronous communication
  \end{itemize}
  Good for implementation.

\item
  Proof construction $\cong$ process-as-formula (Miller et al.),
                             multiset rewriting (Cervesato and Scedrov).
  \begin{itemize}
  \item Strengths: more global description (though slightly lower level than global types?);
                   \emph{a}synchronous communication;
  \item Weaknesses: untyped; can deadlock; less obviously extensible;
                    specification, but not implementation
  \end{itemize}
  Good for specification.

\item
  Where do the proof reduction and proof construction characterizations intersect?
  \begin{itemize}
  \item Intersection will have strengths of both, with weaknesses of neither.
  \item On the intersection, a proof-construction specification can be \enquote{projected} to a proof-reduction implementation.
  \end{itemize}

\item
  To focus our attention on the essential differences between proof reduction and proof construction, we limit our investigation to chain topologies, not the more general tree topologies of SILL.
  \begin{itemize}
  \item Chain topologies let us treat channels namelessly, which turns out to lift a large notational burden.
  \item In both the proof reduction and proof construction chacterizations, there are logical justifications for chain topologies.
  \item We conjecture that our results can be straightforwardly generalized to tree topologies, with a large notational overhead, but no conceptual difficulties.
  \end{itemize}

\item
  Outline of chapters and discussion of contributions

\item
  Abstract rewriting, specifically string rewriting, as a framework for specifying the dynamics of concurrent systems.
  \begin{itemize}
  \item Systems with components arranged in a linear topology with a monoidal structure 
  \item Special case of multiset rewriting (Cervesato and Scedrov), so not a contribution of this work
  \item Introduce running examples for the first time: NFAs and binary counters.
    (A review of finite automata can be found in \cref{ch:finite-automata}.)
  \end{itemize}

\item
  Ordered rewriting generalizes string rewriting to a richer set of \enquote{connectives} -- free residuated lattice, not just a free monoid
  \begin{itemize}
  \item Based on the ordered sequent calculus of \cref{ch:ordered-logic}
  \item Ordered rewriting justified by refactoring the sequent calculus left rules to eliminate boilerplate.
    \begin{itemize}
    \item Rewriting is obtained as a fragment of the refactored calculus
    \item This derivation appears to be an auxiliary contribution of this work
    \end{itemize}
  \item Roughly a special case of (Cervesato and Scedrov)
  \item \emph{Focused} ordered rewriting to control the atomicity of individual rewriting steps
  \end{itemize}

\item 
  Both string rewriting and (focused) ordered rewriting are global, state transformation systems.
  \begin{itemize}
  \item Our goal is to identify a fragment (intersection) that acts like a process calculus -- a local, formula-as-process interpretation.
    \begin{itemize}
    \item Propositions as process expressions, contexts as runtime process configurations, atoms as messages 
    \item Surprisingly few changes needed: atom directions and syntactic restriction placed on implications
    \item Not really a contribution?  (Miller et al.)
    \end{itemize}
  \item coinductively defined propositions, not replication, for unbounded rewriting 
  \item Major contribution of this chapter: proceedure for choreographing string rewriting specifications into formula-as-process ordered rewriting
    \begin{itemize}
    \item Role assignment to each symbol -- message or process
    \item Not all role assignments lead to sensible choreographies
    \item String rewriting axioms used as an evaluation\fixnote{word choice?} -- specification and choreography must be bisimilar.
      [First appearance of a bisimiarity.  States' structures are observable]
    \item Informal description, then formal description 
    \end{itemize}
  \item Choreographies for binary counters and NFAs 
    \begin{itemize}
    \item End-to-end adequacy of choreography as a composition of specification's adequacy with bisimilarity of choreography and specification
    \item NFA example leads to a desire for an equivalence coarser than equality 
    \end{itemize}
  \end{itemize}

\item A notion of bisimilarity for ordered rewriting
  \begin{itemize}
  \item observatinal equivalence: prepositions/processes are opaque; atoms/messages are observable 
    \begin{itemize}
    \item very much dependent on formula-as-process interpretation
    \end{itemize}
  \item Contribution?  Related to (Deng et al.) but our formulation is different
  \item Labeled bisimilarity as a sound (and complete!) proof technique for rewriting bisimilarity
  \item Examples of bisimilarity: NFA encoding preserves bisimilarity; bisimilarity of binary counters coincides with equal denotations 
  \end{itemize}

\item That closes proof construction.  Now for proof reduction.

\item Singleton logic
  \begin{itemize}
  \item Independently identified by (Fortier and Santocanale)
  \end{itemize}
\end{itemize}

 






























































\subsection{}

\begin{itemize}
\item Computation as deduction: clear, expressive, and provably correct programs
  \begin{itemize}
  \item Examples of sucess stories
  \item Can it be applied to concurrency?
  \end{itemize}
\item Proof constuction and proof reduction views of con currency
  \begin{itemize}
  \item Proof construction: good for specifications 
  \item Proof reduction: good for implementions 
  \end{itemize}
\item Thesis statement: Session types bridge these two views
\item Ordered logic as a proving ground 
  \begin{itemize}
  \item Ordered rewriting for proof construction
  \item Singleton logic (purely additive fragment of ordered logic) for proof reduction
  \end{itemize}
\item Ordered rewriting (chapter 4) for specifications
  \begin{itemize}
  \item DFAs and NFAs 
  \item Binary counters
  \end{itemize}
\item Refinement of ordered rewriting for choreographies (chapter 5)
  \begin{itemize}
  \item Recursive definitions as processes; atoms as messages 
  \item Untyped (mostly, except for directions)
  \item Rewriting bisimilarity for observational equivalence
    \begin{itemize}
    \item Examples
    \end{itemize}
  \end{itemize}
\item Singleton logic and its semi-axiomatic calculus (chapter 6)
\item 
\end{itemize}


Concurrent systems are notoriously difficult to get right.

Beginning with Curry's observation that Hilbert [...] corresponds to a form of computation based on combinatory reduction\autocite{??}, and continuing with Howard's discovery of an isomorphism between [Gentzen's] intuitionistic natural deduction and Church's simply-typed $\lambda$-calculus, computation-as-deduction has been the gold standard for clear, expressive, and provably correct programs.

Computation-as-deduction can be divided into two classes: proof-search-as-computation and proof-reduction-as-computation.
The former provides a logically grounded basis for the backward- and forward-chaining logic programming paradigms, whereas the latter is the foundation for the functional programming paradigm.

Logically grounded concurrent computation 

More recently, a proof-reduction description of concurrency has been discovered by \textcite{??} with \textcite{??}.
In this isomorphism, linear propositions correspond to session types; sequent proofs, to session-typed processes; and cut reduction, to synchronous message-passing communication.

This thesis seeks to bring these two apparently divergent views of concurrency together.
Is there a class of specifications for which well-typed implementations can automatically be extracted?

Thesis statement: Session types form the bridge. 


\section{Proposal introduction}

With the increasingly complex, distributed nature of today's software systems, concurrency is ubiquitous.
Concurrency facilitates distributed computation by structuring systems as nondeterministic compositions of simpler subsystems.
But, concomitant with nondeterminism, concurrent systems are notoriously tricky to get right:
subtle races and deadlocks can occur even in the most rigorously tested of systems.

At the same time, decades of research into connections between proof theory and programming languages have firmly established the principle of \vocab{computation as deduction} as the gold standard [framework] for clear, expressive, and provably correct programs.
Most generally, intuitionistic logic is the bedrock for both the typed functional\autocite{Martin-Lof:LMPS80} and logic programming\autocites{Miller+:PAL91}{Andreoli:JLC92}\fixnote{check refs?} paradigms.
In more specific [...]
Examples abound: lax logic for effectful computation\autocite{Benton:JFP98}, temporal logic for functional reactive programming\autocite{Jeffrey:PLPV12}, and linear logic for graph-based algorithms\autocite{Cruz+:ICLP14}, to name just a few.

Can a computation-as-deduction approach make it similarly easier to clearly and concisely specify, as well as correctly implement, concurrent programs?

\subsection{}

The principle of computation as deduction comes in two flavors: \vocab{proof construction as computation} and \vocab{proof reduction as computation}.
Under a proof-construction-as-computation view, the search for a proof, according to a fixed strategy, forms the basis of computation; it is the foundation for logic programming\autocites{Miller+:PAL91}{Andreoli:JLC92}.
The proof-reduction-as-computation view, on the other hand, revolves around a correspondence, known as the Curry--Howard Isomorphism\autocite{Curry:??}{Howard:??}, between propositions and types, proofs and programs, and proof simplification, or reduction, and program evaluation;
it is the foundation for typed functional programming\autocite{Martin-Lof:LMPS80}.

Both the proof-construction and proof-reduction approaches have been successfully applied to concurrent programming, originally stemming from \citeauthor{Girard:TCS87}'s suggestion of connections between linear logic and concurrency\autocite{Girard:TCS87}.
In the proof-construction vein, \acroifusedTF{CLF}{\ac{CLF}\autocite{Watkins+:CMU02}}{the \acf{CLF}\autocite{Watkins+:CMU02}} treats the permutability of inference rules as the source of concurrency.
\Ac{CLF} has been used to specify a variety of concurrent systems, ranging from the $\pi$-calculus to security protocols and even emergent story narratives\autocites{Cervesato+Scedrov:IC09}{Martens+:LPNMR13}.\fixnote{check refs}
Although these same concurrent systems can be simulated according to their \ac{CLF} specifications by the Lollimon\autocite{Lopez+:PPDP05} and Celf\autocite{Schack-Nielsen:ITU11} logic programming engines, the programs ultimately remain specifications, not actual \emph{decentralized} implementations.

Taking the other, proof-reduction tack, \textcite{Abramsky:TCS93}, \textcite{Bellin+Scott:TCS94}, and later \textcite{Caires+Pfenning:CONCUR10} with Toninho\autocites{Caires+:TLDI12}{Caires+:MSCS13} have given correspondences between sequent calculus proofs or proof nets in linear logic and processes; between cut elimination and concurrent process execution.
Moreover, in \citeauthor{Caires+:MSCS13}'s work, the correspondence is a true Curry--Howard isomorphism in that intuitionistic linear propositions are also types -- \vocab{session types}\autocite{Honda:CONCUR93} that describe the interaction protocol to which a process adheres.
Unlike proof construction, the proof-reduction approach yields actual decentralized implementations with independent threads of control\autocites{Toninho+:ESOP13}{Griffith+Pfenning:14}.

In spite of their common basis in linear logic, these proof-construction and proof-reduction approaches to concurrent computation appear at first glance to be strikingly disparate.
They have different dynamics; they offer different guarantees (session fidelity, behavioral type preservation, and deadlock freedom for the proof-reduction approach, but only non-behavioral type preservaton for the proof-construction approach); and, perhaps most importantly, they serve very different roles in programming practice.
Proof construction is better suited to system specification and reasoning, whereas proof reduction is better suited to implementation.

To reduce the possibiity of error when building an implementation from a specification, we would like to minimize the gap between the two.
Despite the apparent disparity between proof construction and proof reduction, is there a class of concurrent specifications from which decentralized concurrent implementations can be automatically extracted?
Stated differently, is there perhaps (some fragment of) a substructural logic in which the computational natures of proof construction and proof reduction coincide?

\subsection{}

The thesis is that, yes, we can indeed have our cake and eat it too:
\begin{quotation}
\normalsize\noindent
Thesis statement.
\itshape Session types form a bridge between distinct notions of concurrency in computational interpretations of ordered logic based on proof construction, on one hand, and proof reduction, on the other hand.
\end{quotation}

\Cref{part:preliminaries} reviews some necessary background information: definitions of nondeterministic and \aclp*{DFA}~\parencref{ch:automata}, and a sequent calculus presentation of ordered logic~\parencref{ch:ordered-logic}.
The reader should feel free to skim or skip these \lcnamecrefs{ch:automata} and return to them 

\newthought{\Cref{part:proof-construction} then delves} into a proof-construction approach to concurrency, beginning with a review of a framework for string rewriting~\parencref{ch:string-rewriting}.
Because disjoint segments of a string may be rewritten independently, string rewriting can be used to specify concurrent systems that have a linear topology.
The \lcnamecref{ch:string-rewriting} concludes with two extended examples of string rewriting specifications:
\aclp*{NFA}\footnote{And, as a special case, deterministic ones, too.}
and binary representations of natural numbers equipped with increment and decrement operations.
These will serve as running examples throughout this document.

Despite being concurrent, string rewriting specifications lack an immediate notion of local execution.
String rewriting presumes the existence of a central conductor that orchestrates the computation, rewriting the string globally.
Global rewriting, although reasonable for concurrent specifications, will not map well to locally executing process implementations that a proof-reduction approach to concurrency suggests -- the gap is simply too large.

Toward this end, \cref{ch:ordered-rewriting} presents an extension of the Lambek calculus\autocite{Lambek:??} in which string rewriting specifications may be refined into \vocab{choreographies}.
Inspired by the process-as-formula view of linear logic\autocites{Miller:??}{Cervesato+Scedrov:IC09}, choreographies exist at a slightly lower level of abstraction than string rewriting specifications in that they begin to introduce a message-passing character.
A choreography partitions a string rewriting specification's symbols into 

Having introduced a message-passing character and stronger notion of process identity, we can then ask when two processes are observationally equivalent.
\Cref{ch:ordered-bisimilarity} develops a notion of bisimilarity for ordered propositions, where the uninterpreted atoms are the only observables.

\subsection{Contributions}

The contributions of this thesis can be viewed from several perspectives.
\begin{itemize}
\item This work can be seen as a proof-theoretic [logical] reconstruction of multiparty session types~\autocite{Honda+:POPL08}.
In multiparty session types, binary session types are generalized to conversations among several parties.
Conversations in their entirety are specified using global session types.
Global types can be projected to binary session types for each pair of participants, which very nearly are implementations.
\item This work can be seen to further understanding of proof construction and proof reduction.
\item Gives types to logic programs.
Guarantees deadlock-freedom.
\end{itemize}
In addition to the practical benefit of 


The remainder of this document aims to establish this thesis as a plausible one.
To do so, we turn our attention from linear logic to (non-modal) intuitionistic ordered logic~\autocites{Lambek:AMM58}{Polakow+Pfenning:MFPS99}---a restriction of linear logic in which the context of hypotheses forms a list rather than a multiset or bag---and defend the thesis in this restricted setting.
The proposed thesis research is to relax the restrictions and expand the ideas in this document to intuitionistic linear logic.

Specifically, this document describes ... as depicted in \cref{fig:outline}.
First, \cref{?} reviews a string rewriting interpretation of proof construction in a [non-modal] fragment of intuitionistic ordered logic~\autocite{Simmons:CMU12}.
String rewriting specifications in this fragment are equipped with a natural notion of concurrency based on treating  as equivalent the different interleavings of independent rewriting steps.
[equivalence classes of proofs.]

Despite being concurrent, these string rewriting specifications lack an immediate notion of \emph{process} or \emph{process identity}.
Toward this end, \cref{?} introduces \vocab{choreographies}, a further restriction of string rewriting specifications obtained when [in which] atomic propositions are assigned roles as either process-like atoms or message-like atoms.
(Message-like atoms, such as  in \cref{fig:outline}, are indicated with an arrow decoration.)
A specification may admit several choreographies, but, as described in \cref{?}, a well-formed choreography must be (lock-step) equivalent with the specification.

Even with process-like atoms, choreographies remain string rewriting specifications, not actual distributed implementations of processes.
Nevertheless, choreographies are a stepping-stone to process implementations. 
In \cref{?}, we develop a session-typed process calculus from a Curry--Howard interpretation of a fragment of linear logic; 
\Cref{?} shows how choreographies can be compiled to 

Choreographies serve as a stepping stone to full-fledged process definitions.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
